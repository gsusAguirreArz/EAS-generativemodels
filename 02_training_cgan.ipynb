{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.10.2\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from time import time\n",
    "\n",
    "import utils.data as du\n",
    "import utils.training as tut\n",
    "from utils.models.cwgangp import Critic, Generator, initialize_weights\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_FREQ = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 256\n",
    "CHANNELS_IMG = 1\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 5\n",
    "FEATURES_CRITIC = 64\n",
    "FEATURES_GEN = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "IMG_SIZE = 9\n",
    "NUM_CLASSES = None\n",
    "GEN_EMBEDDING = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors_signals, arrival_times, logE, mass, xmax = du.get_data()\n",
    "\n",
    "# arrival_times = du.proposedArrivalTimesNorm(arrival_times)\n",
    "shower_maps = du.arrivalTimesDomMap(arrival_times)\n",
    "shower_maps = shower_maps.reshape(-1, 1, 9, 9)\n",
    "# total_signals = du.proposedTotalSignals(detectors_signals)\n",
    "\n",
    "plot = du.plot_signals_arrival_times(arrival_times, N=3, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "mass = encoder.fit_transform(mass)\n",
    "NUM_CLASSES = len(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(shower_maps).type(torch.float32)\n",
    "y = torch.from_numpy(mass).type(torch.int64)\n",
    "\n",
    "train_set = TensorDataset(x, y)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "writer_fake = SummaryWriter(f\"logs/CWGANGP_EAS/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/CWGANGP_EAS/real\")\n",
    "writer_scalar = SummaryWriter(f\"logs/CWGANGP_EAS/loses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block name: Conv-Generator\n",
      "----------------------------------------\n",
      " | LayerName | \t | Size | \t | Nparams | \n",
      "======================================================================\n",
      "gen_net.0.0.weight: \t (256, 256, 3, 3), \t 589824\n",
      "gen_net.0.1.weight: \t (256,), \t 256\n",
      "gen_net.0.1.bias: \t (256,), \t 256\n",
      "gen_net.0.1.running_mean: \t (256,), \t 256\n",
      "gen_net.0.1.running_var: \t (256,), \t 256\n",
      "gen_net.0.1.num_batches_tracked: \t (), \t 1\n",
      "gen_net.1.0.weight: \t (256, 256, 3, 3), \t 589824\n",
      "gen_net.1.1.weight: \t (256,), \t 256\n",
      "gen_net.1.1.bias: \t (256,), \t 256\n",
      "gen_net.1.1.running_mean: \t (256,), \t 256\n",
      "gen_net.1.1.running_var: \t (256,), \t 256\n",
      "gen_net.1.1.num_batches_tracked: \t (), \t 1\n",
      "gen_net.2.0.weight: \t (256, 128, 3, 3), \t 294912\n",
      "gen_net.2.1.weight: \t (128,), \t 128\n",
      "gen_net.2.1.bias: \t (128,), \t 128\n",
      "gen_net.2.1.running_mean: \t (128,), \t 128\n",
      "gen_net.2.1.running_var: \t (128,), \t 128\n",
      "gen_net.2.1.num_batches_tracked: \t (), \t 1\n",
      "gen_net.3.0.weight: \t (128, 128, 3, 3), \t 147456\n",
      "gen_net.3.1.weight: \t (128,), \t 128\n",
      "gen_net.3.1.bias: \t (128,), \t 128\n",
      "gen_net.3.1.running_mean: \t (128,), \t 128\n",
      "gen_net.3.1.running_var: \t (128,), \t 128\n",
      "gen_net.3.1.num_batches_tracked: \t (), \t 1\n",
      "gen_net.4.0.weight: \t (128, 1, 3, 3), \t 1152\n",
      "gen_net.4.1.weight: \t (1,), \t 1\n",
      "gen_net.4.1.bias: \t (1,), \t 1\n",
      "gen_net.4.1.running_mean: \t (1,), \t 1\n",
      "gen_net.4.1.running_var: \t (1,), \t 1\n",
      "gen_net.4.1.num_batches_tracked: \t (), \t 1\n",
      "embed.weight: \t (4, 128), \t 512\n",
      "======================================================================\n",
      "Total parameters: 1625218\n",
      "Trainable parameters: 1625218\n",
      "Non-trianable parameters: 0\n",
      "\n",
      "Block name: Conv-Critic\n",
      "----------------------------------------\n",
      " | LayerName | \t | Size | \t | Nparams | \n",
      "======================================================================\n",
      "critic_net.0.0.weight: \t (64, 2, 3, 3), \t 1152\n",
      "critic_net.0.1.weight: \t (64,), \t 64\n",
      "critic_net.0.1.bias: \t (64,), \t 64\n",
      "critic_net.1.0.weight: \t (128, 64, 3, 3), \t 73728\n",
      "critic_net.1.1.weight: \t (128,), \t 128\n",
      "critic_net.1.1.bias: \t (128,), \t 128\n",
      "critic_net.2.0.weight: \t (128, 128, 3, 3), \t 147456\n",
      "critic_net.2.1.weight: \t (128,), \t 128\n",
      "critic_net.2.1.bias: \t (128,), \t 128\n",
      "critic_net.3.0.weight: \t (256, 128, 3, 3), \t 294912\n",
      "critic_net.3.1.weight: \t (256,), \t 256\n",
      "critic_net.3.1.bias: \t (256,), \t 256\n",
      "critic_net.4.0.weight: \t (256, 256, 3, 3), \t 589824\n",
      "critic_net.4.1.weight: \t (256,), \t 256\n",
      "critic_net.4.1.bias: \t (256,), \t 256\n",
      "critic_net.5.weight: \t (1, 256, 3, 3), \t 2304\n",
      "critic_net.5.bias: \t (1,), \t 1\n",
      "embed.weight: \t (4, 81), \t 324\n",
      "======================================================================\n",
      "Total parameters: 1111365\n",
      "Trainable parameters: 1111365\n",
      "Non-trianable parameters: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_model = Generator(LATENT_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING, \"Conv-Generator\").to(device)\n",
    "critic_model = Critic(CHANNELS_IMG, FEATURES_CRITIC, NUM_CLASSES, IMG_SIZE, \"Conv-Critic\").to(device)\n",
    "fixed_noise = torch.randn(BATCH_SIZE, LATENT_DIM, 1, 1).to(device)\n",
    "initialize_weights(gen_model)\n",
    "initialize_weights(critic_model)\n",
    "\n",
    "for model in [gen_model, critic_model]:\n",
    "    tut.print_model_summary(model)\n",
    "\n",
    "opt_gen = optim.Adam(gen_model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9) )\n",
    "opt_critic = optim.Adam(critic_model.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.9) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======================================================================>] 100%, batch 391 \t\n",
      "--------------------------------------------------------------------------------------\n",
      "| Epoch: [001/005] | Loss Critic: -4.3295 | Loss G: 7.9772 | time: 80.98s | \n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "[======================================================================>] 100%, batch 391 \t\n",
      "--------------------------------------------------------------------------------------\n",
      "| Epoch: [003/005] | Loss Critic: -3.4920 | Loss G: 9.1175 | time: 79.95s | \n",
      "--------------------------------------------------------------------------------------\n",
      "\n",
      "[======================================================================>] 100%, batch 391 \t\n",
      "--------------------------------------------------------------------------------------\n",
      "| Epoch: [005/005] | Loss Critic: -2.8068 | Loss G: 9.6875 | time: 78.71s | \n",
      "--------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "gen_model.train()\n",
    "critic_model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time()\n",
    "    for batch_idx, (real, labels) in enumerate(train_loader):\n",
    "        real = real.to(device)\n",
    "        current_batch_size = real.shape[0]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Train critic: max E[critic(real)] - E[critic(fake)]\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(current_batch_size, LATENT_DIM, 1, 1).to(device)\n",
    "            fake = gen_model(noise, labels)\n",
    "            critic_real = critic_model(real, labels).reshape(-1)\n",
    "            critic_fake = critic_model(fake, labels).reshape(-1)\n",
    "            gp = tut.gradient_penalty(critic_model, real, fake, device=device, labels=labels)\n",
    "            loss_critic = ( -(torch.mean(critic_real) - torch.mean(critic_fake)) + LAMBDA_GP * gp )\n",
    "            critic_model.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "        \n",
    "        # Train generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic_model(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen_model.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        tut.print_batches_progress_bar(batch_idx, len(train_loader))\n",
    "        \n",
    "        info_dict = {\"Loss Critic\": loss_critic, \"Loss G\": loss_gen}\n",
    "\n",
    "        gen_model.eval()\n",
    "        critic_model.eval()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            with torch.no_grad():\n",
    "                fake = gen_model(noise,labels)\n",
    "                data = real\n",
    "\n",
    "                fake = fake.cpu().numpy()\n",
    "                data = data.cpu().numpy()\n",
    "\n",
    "                fake_showers_fig = du.plot_signals_arrival_times(fake, N=3 )\n",
    "                real_showers_fig = du.plot_signals_arrival_times(data, N=3 )\n",
    "\n",
    "                writer_fake.add_figure(\"Fake showers\", fake_showers_fig, global_step=epoch)\n",
    "                writer_real.add_figure(\"Real showers\", real_showers_fig, global_step=epoch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            writer_scalar.add_scalar(\"Loss/Critic\", loss_critic.cpu().item(), global_step=step )\n",
    "            writer_scalar.add_scalar(\"Loss/Gen\", loss_gen.cpu().item() , global_step=step )\n",
    "            \n",
    "        gen_model.train()\n",
    "        critic_model.train()\n",
    "        step += 1\n",
    "    \n",
    "    elapsed_time = time() - start_time\n",
    "    info_dict[\"time\"] = elapsed_time\n",
    "    tut.print_epoch_info(epoch, EPOCHS, INFO_FREQ, info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 7578), started 0:00:04 ago. (Use '!kill 7578' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-97ba55e33c8a3144\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-97ba55e33c8a3144\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs/CWGANGP_EAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54790be7feb9a1f7f5848bc94d056ddc0d573b6fc2cf4588b55f07e607fa503c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
